p8105_hw6_mk5149
================
Mungyu Kwok
2025-11-25

# Preparation

Load any necessary libraries for this homework

``` r
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.4     ✔ readr     2.1.5
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.1
    ## ✔ ggplot2   3.5.2     ✔ tibble    3.3.0
    ## ✔ lubridate 1.9.4     ✔ tidyr     1.3.1
    ## ✔ purrr     1.1.0     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(broom)
library(modelr)
```

    ## 
    ## Attaching package: 'modelr'
    ## 
    ## The following object is masked from 'package:broom':
    ## 
    ##     bootstrap

# Problem 1: Washington Prosts Homicides

## Problem 1 Part 1: Data loading and cleaning

The analysis begins by loading the raw data `homicide-data.csv`. The
data is cleaned according to the problem specifications: creating a
`city_state` variable, creating a binary solved variable indicating
whether the homicide is solved, omitting specified cities (Dallas, TX;
Phoenix, AZ; and Kansas City, MO; Tulsa, AL), limiting to `White` and
`Black` victims, and ensuring `victim_age` is numeric.

``` r
# Load the data
homicides = read_csv("data/homicide-data.csv") 
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
homicides_cleaned = homicides |>
  # Step 1: create city_state variable
  mutate(city_state = paste(city, state, sep = ", "),
         # Step 2: create a binary solved variable: 1 for closed by arrest, 0 for unresoved
         solved = if_else(disposition == "Closed by arrest", 1, 0),
         # Step 3: ensure victim_age is numerice
         victim_age = as.numeric(victim_age)
  ) |>
  # Step 4: omit specified cities
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")) |>
  # Step 5: limit to white or black victims only
  filter(victim_race %in% c("White", "Black")) |>
  # Step 6: remove rows with missing age
  drop_na(victim_age) |>
  # Step 7: Convert victim_sex to a factor, setting 'Female' as the reference level
  mutate(victim_sex = fct_relevel(victim_sex, "Female"))
```

    ## Warning: There was 1 warning in `mutate()`.
    ## ℹ In argument: `victim_age = as.numeric(victim_age)`.
    ## Caused by warning:
    ## ! NAs introduced by coercion

``` r
# Check "solved" binary variable
homicides_cleaned |> 
  pull(solved) |> 
  unique()
```

    ## [1] 0 1

``` r
homicides_cleaned |>
  head()
```

    ## # A tibble: 6 × 14
    ##   uid   reported_date victim_last victim_first victim_race victim_age victim_sex
    ##   <chr>         <dbl> <chr>       <chr>        <chr>            <dbl> <fct>     
    ## 1 Alb-…      20100601 SATTERFIELD VIVIANA      White               15 Female    
    ## 2 Alb-…      20100102 MULA        VIVIAN       White               72 Female    
    ## 3 Alb-…      20100126 BOOK        GERALDINE    White               91 Female    
    ## 4 Alb-…      20100130 MARTIN-LEY… GUSTAVO      White               56 Male      
    ## 5 Alb-…      20100308 GRAY        STEFANIA     White               43 Female    
    ## 6 Alb-…      20100323 DAVID       LARRY        White               52 Male      
    ## # ℹ 7 more variables: city <chr>, state <chr>, lat <dbl>, lon <dbl>,
    ## #   disposition <chr>, city_state <chr>, solved <dbl>

## Problem 1 Part 2: Logiestic regression for Baltimore, MD

For the city of Baltimore, MD, use the `glm` function to fit a logistic
regression with resolved vs unresolved as the outcome and victim age,
sex and race as predictors. Save the output of `glm` as an R object;
apply the `broom::tidy` to this object; and obtain the estimate and
confidence interval of the adjusted odds ratio for solving homicides
comparing male victims to female victims keeping all other variables
fixed.

``` r
# Filter data fro Baltimore, MD 
baltimore_df = homicides_cleaned |>
  filter(city_state == "Baltimore, MD")

# Fit the logistic regression model
glm_baltimore = glm(solved ~ victim_age + victim_sex + victim_race, 
                    data = baltimore_df,
                    family = binomial)

# Tidy the results
# Exponentiate to get ORs, extracts the required values
or_baltimore = tidy(glm_baltimore, conf.int = TRUE, exponentiate = TRUE) |>
  filter(term == "victim_sexMale") |>
  select(term, estimate, conf.low, conf.high)

or_baltimore
```

    ## # A tibble: 1 × 4
    ##   term           estimate conf.low conf.high
    ##   <chr>             <dbl>    <dbl>     <dbl>
    ## 1 victim_sexMale    0.426    0.324     0.558

The adjusted odds ratio for solving homicides comparing male victims to
female victims in Baltimore, MD is approximately $\mathbf{0.426}$, with
a $95\%$ confidence interval of $(\mathbf{0.325}, \mathbf{0.558})$. This
indicates that the odds of a homicide being solved for male victims are
significantly lower than for female victims, controlling for age and
race.

## Problem 1 Part 3: Multi-City Regression and Tidy Pipeline

Now run `glm` for each of the cities in dataset, and extract the
adjusted odds ratio (and CI) for solving homicides comparing male
victims to female victims. Do this within a “tidy” pipeline, making use
of `purrr::map`, list columns, and unnest as necessary to create a
dataframe with estimated ORs and CIs for each city.

``` r
# Group the data by city_state and nest it
nested_data = homicides_cleaned |>
  group_by(city_state) |>
  nest()

# Define the function to fit GLM and extract OR/CI
get_male_or = function(df) {
  # Requires at least 2 levels for sex and race to fit the full model
  if(n_distinct(df$victim_sex) < 2 | n_distinct(df$victim_race) < 2) {
    return(NULL)
  }
  
  # Fit the logistic regression model
  glm_model = glm(solved ~ victim_age + victim_sex + victim_race,
                  data = df,
                  family = binomial)
  
  # Check for convergence
  if(!glm_model$converged) {
    return(NULL)
  }
  
  # Step 4: Tidy the result and extract the OR for male vs female
  tidy_result = tidy(glm_model, conf.int = TRUE, exponentiate = TRUE) |>
    filter(term == "victim_sexMale") |>
    select(OR = estimate, CI_low = conf.low, CI_high = conf.high)
    
  # Step 5: FIX: Check for perfect separation instability (Inf bounds)
  # Filter out models where OR or CI bounds are unstable (Inf/NaN)
  if (is.infinite(tidy_result$OR) |
      is.infinite(tidy_result$CI_low) |
      is.infinite(tidy_result$CI_high)) {
    return(NULL)
  }

  # Step 6: Return stable results
  return(tidy_result)
}

# Apply the function across all nested dataframes and unnest the results
all_cities_or_ci = nested_data |>
  # Use map to apply the function to each city's data
  mutate(or_ci = map(data, get_male_or)) |>
  # Remove rows where the model failed to fit (or_ci = Null)
  filter(!map_lgl(or_ci, is.null)) |>
  # Unnest the ORs/CIs into a single dataframe
  unnest(or_ci) |>
  select(city_state, OR, CI_low, CI_high) |>
  # Sort by OR for plotting
  arrange(desc(OR))
```

    ## Warning: There were 43 warnings in `mutate()`.
    ## The first warning was:
    ## ℹ In argument: `or_ci = map(data, get_male_or)`.
    ## ℹ In group 1: `city_state = "Albuquerque, NM"`.
    ## Caused by warning:
    ## ! glm.fit: fitted probabilities numerically 0 or 1 occurred
    ## ℹ Run `dplyr::last_dplyr_warnings()` to see the 42 remaining warnings.

``` r
all_cities_or_ci
```

    ## # A tibble: 47 × 4
    ## # Groups:   city_state [47]
    ##    city_state           OR CI_low CI_high
    ##    <chr>             <dbl>  <dbl>   <dbl>
    ##  1 Albuquerque, NM   1.77   0.825    3.76
    ##  2 Stockton, CA      1.35   0.626    2.99
    ##  3 Fresno, CA        1.34   0.567    3.05
    ##  4 Nashville, TN     1.03   0.681    1.56
    ##  5 Richmond, VA      1.01   0.483    1.99
    ##  6 Atlanta, GA       1.00   0.680    1.46
    ##  7 Tulsa, OK         0.976  0.609    1.54
    ##  8 Oklahoma City, OK 0.974  0.623    1.52
    ##  9 Minneapolis, MN   0.947  0.476    1.88
    ## 10 Indianapolis, IN  0.919  0.678    1.24
    ## # ℹ 37 more rows

``` r
#Check
all_cities_or_ci |> 
  nrow()
```

    ## [1] 47

``` r
# Any inf?
all_cities_or_ci |> 
  filter(is.infinite(OR) | is.infinite(CI_low) | is.infinite(CI_high))
```

    ## # A tibble: 0 × 4
    ## # Groups:   city_state [0]
    ## # ℹ 4 variables: city_state <chr>, OR <dbl>, CI_low <dbl>, CI_high <dbl>

## Problem 1 Part 4: Visualization and Comment

Create a plot that shows the estimated ORs and CIs for each city.
Organize cities according to estimated OR, and comment on the plot. Here
a forest plot is created using ggplot2 to visualize the city-specific
$\text{OR}_{\text{Male vs. Female}}$ estimates and $95\%$ CIs, sorted by
the estimated OR.

``` r
# Create the plot
all_cities_or_ci |>
  ggplot(aes(x = OR, y = fct_reorder(city_state, OR))) +
  # Add the vertical line for OR = 1, which is 'no effect'
  geom_vline(xintercept = 1, linetype = "dashed", color = "red") +
  # Plot the ORs and CIs
  geom_errorbarh(aes(xmin = CI_low, xmax = CI_high), height = 0.2, color = "darkblue") +
  geom_point(color = "darkblue", size = 2) +
  # Set labels and title
  labs(
    x = "Adjusted Odds Ratio (Male vs. Female)",
    y = "City, State",
    title = "Adjusted Odds Ratio and 95% CI for Solving Homicides",
    subtitle = "Male vs. Female Victims (Adjusted for Age and Race) across 47 U.S. Cities"
  ) +
  theme_minimal() +
  theme(plot.title.position = "plot")
```

![](p8105_hw6_mk5149_files/figure-gfm/unnamed-chunk-5-1.png)<!-- -->

``` r
ggsave("male_vs_female_or_plot.png", width = 8, height = 10)
```

**Comment on the Plot**:

1.  *Systematic Disparity*: The plot reveals a strong, systematic
    pattern: the majority of estimated odds ratios ($\text{OR}$s) are
    less than $1$. This means that in nearly all cities, controlling for
    age and race, the odds of a homicide being solved are lower for male
    victims compared to female victims.

2.  *Statistically Significant Differences*: Many cities, including
    Baltimore, MD ($\text{OR} \approx 0.43$) and Newark, NJ
    ($\text{OR} \approx 0.44$), show confidence intervals that do not
    overlap with $1$. This indicates a statistically significant
    disparity where male victims are less likely to have their homicides
    solved.

3.  *Ambiguity*: Only a few cities, such as Albuquerque, NM
    ($\text{OR} \approx 1.77$), show an estimated OR greater than $1$.
    However, their confidence intervals cross $1$, meaning the observed
    difference is not statistically significant in these locations.

# Problem 2: Bootstrap Inference for Central Park Weather Data

## Problem 2 Part 1: Import the data

``` r
library(p8105.datasets)
data("weather_df")

weather_df = weather_df |>
drop_na(tmax, tmin, prcp)
```

This problem aims to estimate the distribution and $95\%$ confidence
intervals for two quantities derived from a linear model
($\text{tmax} \sim \text{tmin} + \text{prcp}$) using 5000 bootstrap
samples:

1.  The coefficient of determination, $\hat{r}^2$.

2.  The product of the two slope coefficients,
    $\hat{\beta}_1 \hat{\beta}_2$ (where $\hat{\beta}_1$ is the
    coefficient for $\text{tmin}$ and $\hat{\beta}_2$ is the coefficient
    for $\text{prcp}$).

We will use 5000 bootstrap samples to estimate the sampling
distributions and $95\%$ confidence intervals for $\hat{r}^2$ and
$\hat{\beta}_1 / \hat{\beta}_2$.

## Problem 2 Part 2: **Bootstrap Estimation**

We’ll use `modelr::bootstrap` to generate the 5000 bootstrap samples and
apply a custom function to fit the model and extract the two required
quantities for each sample.

``` r
# Function to fit the model and extract r^2 and the ratio of coefficients
get_metrics = function(df) {
  # Fit the linear model
  lm_model = lm(tmax ~ tmin + prcp, data = df)
  
  # Extract r-squared using broom::glance
  r_squared = glance(lm_model) |> 
    pull(r.squared)
  
  # Extract coefficients using broom::tidy
  coeffs = tidy(lm_model) |> 
    # Select only the slope coefficients
    filter(term %in% c("tmin", "prcp")) |>
    # Pivot wider to get both estimates on the same row
    select(term, estimate) |>
    pivot_wider(names_from = term, values_from = estimate)
  
  # Calculate the ratio beta_1 / beta_2 (tmin / prcp)
  ratio = coeffs$tmin / coeffs$prcp
  
  # Return a single row tibble
  tibble(r_squared = r_squared, beta_ratio = ratio)
}

# Perform the 5000 bootstrap iterations
set.seed(42) # For reproducibility
boot_results = weather_df |>
  modelr::bootstrap(n = 5000) |>
  # Apply the function to each bootstrap sample's data
  mutate(metrics = map(strap, get_metrics)) |>
  # Unnest the results into a single data frame
  unnest(metrics)

# Check the first few results
boot_results |>
  select(-strap) 
```

    ## # A tibble: 5,000 × 3
    ##    .id   r_squared beta_ratio
    ##    <chr>     <dbl>      <dbl>
    ##  1 0001      0.940      -175.
    ##  2 0002      0.943      -242.
    ##  3 0003      0.942      -202.
    ##  4 0004      0.943      -164.
    ##  5 0005      0.935      -209.
    ##  6 0006      0.938      -215.
    ##  7 0007      0.940      -143.
    ##  8 0008      0.943      -179.
    ##  9 0009      0.945      -142.
    ## 10 0010      0.939      -238.
    ## # ℹ 4,990 more rows

``` r
# |> head()
```

------------------------------------------------------------------------

## Problem 2 Part 3: **Distribution of Estimates**

### **Visualizing the Distributions**

Histograms are used to visualize the sampling distributions of
$\hat{r}^2$ and $\hat{\beta}_1 / \hat{\beta}_2$.

``` r
# Combine the two metrics into a long format for plotting
plot_df = boot_results |>
  select(r_squared, beta_ratio) |>
  pivot_longer(
    cols = everything(), 
    names_to = "metric", 
    values_to = "value"
  ) |>
  # Create a clean label for the facets
  mutate(metric_label = if_else(metric == "r_squared", "Coefficient of Determination (R^2)", "Ratio of Coefficients (beta_1 / beta_2)"))

# Create the two-panel histogram plot
ggplot(plot_df, aes(x = value)) +
  geom_histogram(bins = 30, fill = "lightblue", color = "darkblue", alpha = 0.7) +
  facet_wrap(~ metric_label, scales = "free") +
  labs(
    title = "Bootstrap Distributions of R-squared and the Beta Ratio",
    x = "Estimated Value",
    y = "Count"
  ) +
  theme_minimal() +
  theme(plot.title.position = "plot")
```

![](p8105_hw6_mk5149_files/figure-gfm/unnamed-chunk-8-1.png)<!-- -->

``` r
ggsave("bootstrap_distributions.png", width = 10, height = 5)
```

### **Description of the Distributions**

- **Coefficient of Determination ($\hat{r}^2$):** The distribution of
  $\hat{r}^2$ is **highly concentrated** and appears **skewed slightly
  to the left (negatively skewed)**. The bulk of the estimates falls in
  a narrow range around $0.935$ to $0.945$. This strong concentration
  indicates that the linear model consistently explains a very high
  proportion of the variance in $\text{tmax}$ across the different
  bootstrap samples.

- **Ratio of Coefficients ($\hat{\beta}_1 / \hat{\beta}_2$):** The
  distribution of the ratio is **much wider** and appears roughly
  **symmetrical and bell-shaped**, suggesting a distribution close to
  normal. The estimates are centered around a value close to $-150$.
  This wider spread, compared to $\hat{r}^2$, reflects greater sampling
  variability for the ratio of two estimated regression parameters.

------------------------------------------------------------------------

## Problem 2 Part 4: **$95\%$ Confidence Intervals**

The $95\%$ confidence intervals are obtained by finding the $2.5\%$ and
$97.5\%$ quantiles of the 5000 bootstrap estimates.

``` r
# Calculate the 2.5% and 97.5% quantiles for both metrics
confidence_intervals = boot_results |>
  summarise(
    r2_ci_low = quantile(r_squared, 0.025),
    r2_ci_high = quantile(r_squared, 0.975),
    ratio_ci_low = quantile(beta_ratio, 0.025),
    ratio_ci_high = quantile(beta_ratio, 0.975)
  )

confidence_intervals
```

    ## # A tibble: 1 × 4
    ##   r2_ci_low r2_ci_high ratio_ci_low ratio_ci_high
    ##       <dbl>      <dbl>        <dbl>         <dbl>
    ## 1     0.935      0.947        -281.         -125.

| Metric | $2.5\%$ Quantile (Lower Bound) | $97.5\%$ Quantile (Upper Bound) |
|:---|:---|:---|
| $\hat{r}^2$ | $\mathbf{0.935}$ | $\mathbf{0.947}$ |
| $\hat{\beta}_1 / \hat{\beta}_2$ | $\mathbf{-280.8434}$ | $\mathbf{-125.0654}$ |

**Conclusion:**

- The **$95\%$ confidence interval for $\hat{r}^2$** is
  $(\mathbf{0.935}, \mathbf{0.947})$.
- The **$95\%$ confidence interval for $\hat{\beta}_1 / \hat{\beta}_2$**
  is $(\mathbf{-280.8434}, \mathbf{-125.0654})$.

# Problem 3
