---
title: "p8105_hw6_mk5149"
output: github_document
date: "2025-11-25"
author: "Mungyu Kwok"
---

# Preparation
Load any necessary libraries for this homework
```{r}
library(tidyverse)
library(broom)
library(modelr)
```

# Problem 1: Washington Prosts Homicides
## Problem 1 Part 1: Data loading and cleaning
The analysis begins by loading the raw data `homicide-data.csv`. The data is cleaned according to the problem specifications: creating a `city_state` variable, creating a binary solved variable indicating whether the homicide is solved, omitting specified cities (Dallas, TX; Phoenix, AZ; and Kansas City, MO; Tulsa, AL), limiting to `White` and `Black` victims, and ensuring `victim_age` is numeric.

```{r}
# Load the data
homicides = read_csv("data/homicide-data.csv") 

homicides_cleaned = homicides |>
  # Step 1: create city_state variable
  mutate(city_state = paste(city, state, sep = ", "),
         # Step 2: create a binary solved variable: 1 for closed by arrest, 0 for unresoved
         solved = if_else(disposition == "Closed by arrest", 1, 0),
         # Step 3: ensure victim_age is numerice
         victim_age = as.numeric(victim_age)
  ) |>
  # Step 4: omit specified cities
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")) |>
  # Step 5: limit to white or black victims only
  filter(victim_race %in% c("White", "Black")) |>
  # Step 6: remove rows with missing age
  drop_na(victim_age) |>
  # Step 7: Convert victim_sex to a factor, setting 'Female' as the reference level
  mutate(victim_sex = fct_relevel(victim_sex, "Female"))

# Check "solved" binary variable
homicides_cleaned |> 
  pull(solved) |> 
  unique()

homicides_cleaned |>
  head()
  
```

## Problem 1 Part 2: Logiestic regression for Baltimore, MD
For the city of Baltimore, MD, use the `glm` function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors. 
Save the output of `glm` as an R object; apply the `broom::tidy` to this object; and obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.
```{r}
# Filter data fro Baltimore, MD 
baltimore_df = homicides_cleaned |>
  filter(city_state == "Baltimore, MD")

# Fit the logistic regression model
glm_baltimore = glm(solved ~ victim_age + victim_sex + victim_race, 
                    data = baltimore_df,
                    family = binomial)

# Tidy the results
# Exponentiate to get ORs, extracts the required values
or_baltimore = tidy(glm_baltimore, conf.int = TRUE, exponentiate = TRUE) |>
  filter(term == "victim_sexMale") |>
  select(term, estimate, conf.low, conf.high)

or_baltimore
```
The adjusted odds ratio for solving homicides comparing male victims to female victims in Baltimore, MD is approximately $\mathbf{0.426}$, with a $95\%$ confidence interval of $(\mathbf{0.325}, \mathbf{0.558})$. This indicates that the odds of a homicide being solved for male victims are significantly lower than for female victims, controlling for age and race.

## Problem 1 Part 3: Multi-City Regression and Tidy Pipeline
Now run `glm` for each of the cities in dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. 
Do this within a “tidy” pipeline, making use of `purrr::map`, list columns, and unnest as necessary to create a dataframe with estimated ORs and CIs for each city.
```{r}
# Group the data by city_state and nest it
nested_data = homicides_cleaned |>
  group_by(city_state) |>
  nest()

# Define the function to fit GLM and extract OR/CI
get_male_or = function(df) {
  # Requires at least 2 levels for sex and race to fit the full model
  if(n_distinct(df$victim_sex) < 2 | n_distinct(df$victim_race) < 2) {
    return(NULL)
  }
  
  # Fit the logistic regression model
  glm_model = glm(solved ~ victim_age + victim_sex + victim_race,
                  data = df,
                  family = binomial)
  
  # Check for convergence
  if(!glm_model$converged) {
    return(NULL)
  }
  
  # Step 4: Tidy the result and extract the OR for male vs female
  tidy_result = tidy(glm_model, conf.int = TRUE, exponentiate = TRUE) |>
    filter(term == "victim_sexMale") |>
    select(OR = estimate, CI_low = conf.low, CI_high = conf.high)
    
  # Step 5: FIX: Check for perfect separation instability (Inf bounds)
  # Filter out models where OR or CI bounds are unstable (Inf/NaN)
  if (is.infinite(tidy_result$OR) |
      is.infinite(tidy_result$CI_low) |
      is.infinite(tidy_result$CI_high)) {
    return(NULL)
  }

  # Step 6: Return stable results
  return(tidy_result)
}

# Apply the function across all nested dataframes and unnest the results
all_cities_or_ci = nested_data |>
  # Use map to apply the function to each city's data
  mutate(or_ci = map(data, get_male_or)) |>
  # Remove rows where the model failed to fit (or_ci = Null)
  filter(!map_lgl(or_ci, is.null)) |>
  # Unnest the ORs/CIs into a single dataframe
  unnest(or_ci) |>
  select(city_state, OR, CI_low, CI_high) |>
  # Sort by OR for plotting
  arrange(desc(OR))

all_cities_or_ci

#Check
all_cities_or_ci |> 
  nrow()

# Any inf?
all_cities_or_ci |> 
  filter(is.infinite(OR) | is.infinite(CI_low) | is.infinite(CI_high))

```

## Problem 1 Part 4: Visualization and Comment
Create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR, and comment on the plot.
Here a forest plot is created using ggplot2 to visualize the city-specific $\text{OR}_{\text{Male vs. Female}}$ estimates and $95\%$ CIs, sorted by the estimated OR.
```{r}
# Create the plot
all_cities_or_ci |>
  ggplot(aes(x = OR, y = fct_reorder(city_state, OR))) +
  # Add the vertical line for OR = 1, which is 'no effect'
  geom_vline(xintercept = 1, linetype = "dashed", color = "red") +
  # Plot the ORs and CIs
  geom_errorbarh(aes(xmin = CI_low, xmax = CI_high), height = 0.2, color = "darkblue") +
  geom_point(color = "darkblue", size = 2) +
  # Set labels and title
  labs(
    x = "Adjusted Odds Ratio (Male vs. Female)",
    y = "City, State",
    title = "Adjusted Odds Ratio and 95% CI for Solving Homicides",
    subtitle = "Male vs. Female Victims (Adjusted for Age and Race) across 47 U.S. Cities"
  ) +
  theme_minimal() +
  theme(plot.title.position = "plot")

ggsave("male_vs_female_or_plot.png", width = 8, height = 10)
  
```
**Comment on the Plot**: 

1. *Systematic Disparity*: The plot reveals a strong, systematic pattern: the majority of estimated odds ratios ($\text{OR}$s) are less than $1$. This means that in nearly all cities, controlling for age and race, the odds of a homicide being solved are lower for male victims compared to female victims.

2. *Statistically Significant Differences*: Many cities, including Baltimore, MD ($\text{OR} \approx 0.43$) and Newark, NJ ($\text{OR} \approx 0.44$), show confidence intervals that do not overlap with $1$. This indicates a statistically significant disparity where male victims are less likely to have their homicides solved.

3. *Ambiguity*: Only a few cities, such as Albuquerque, NM ($\text{OR} \approx 1.77$), show an estimated OR greater than $1$. However, their confidence intervals cross $1$, meaning the observed difference is not statistically significant in these locations.


# Problem 2: Bootstrap Inference for Central Park Weather Data
## Problem 2 Part 1: Import the data
```{r}
library(p8105.datasets)
data("weather_df")

weather_df = weather_df |>
drop_na(tmax, tmin, prcp)

```

This problem aims to estimate the distribution and $95\%$ confidence intervals for two quantities derived from a linear model ($\text{tmax} \sim \text{tmin} + \text{prcp}$) using 5000 bootstrap samples:

1. The coefficient of determination, $\hat{r}^2$.

2. The product of the two slope coefficients, $\hat{\beta}_1 \hat{\beta}_2$ (where $\hat{\beta}_1$ is the coefficient for $\text{tmin}$ and $\hat{\beta}_2$ is the coefficient for $\text{prcp}$).

We will use 5000 bootstrap samples to estimate the sampling distributions and $95\%$ confidence intervals for $\hat{r}^2$ and $\hat{\beta}_1 / \hat{\beta}_2$.

## Problem 2 Part 2: **Bootstrap Estimation**

We'll use `modelr::bootstrap` to generate the 5000 bootstrap samples and apply a custom function to fit the model and extract the two required quantities for each sample.

```{r}
# Function to fit the model and extract r^2 and the ratio of coefficients
get_metrics = function(df) {
  # Fit the linear model
  lm_model = lm(tmax ~ tmin + prcp, data = df)
  
  # Extract r-squared using broom::glance
  r_squared = glance(lm_model) |> 
    pull(r.squared)
  
  # Extract coefficients using broom::tidy
  coeffs = tidy(lm_model) |> 
    # Select only the slope coefficients
    filter(term %in% c("tmin", "prcp")) |>
    # Pivot wider to get both estimates on the same row
    select(term, estimate) |>
    pivot_wider(names_from = term, values_from = estimate)
  
  # Calculate the ratio beta_1 / beta_2 (tmin / prcp)
  ratio = coeffs$tmin / coeffs$prcp
  
  # Return a single row tibble
  tibble(r_squared = r_squared, beta_ratio = ratio)
}

# Perform the 5000 bootstrap iterations
set.seed(42) # For reproducibility
boot_results = weather_df |>
  modelr::bootstrap(n = 5000) |>
  # Apply the function to each bootstrap sample's data
  mutate(metrics = map(strap, get_metrics)) |>
  # Unnest the results into a single data frame
  unnest(metrics)

# Check the first few results
boot_results |>
  select(-strap) 
# |> head()
```

-----

## Problem 2 Part 3: **Distribution of Estimates**

### **Visualizing the Distributions**

Histograms are used to visualize the sampling distributions of $\hat{r}^2$ and $\hat{\beta}_1 / \hat{\beta}_2$.

```{r}
# Combine the two metrics into a long format for plotting
plot_df = boot_results |>
  select(r_squared, beta_ratio) |>
  pivot_longer(
    cols = everything(), 
    names_to = "metric", 
    values_to = "value"
  ) |>
  # Create a clean label for the facets
  mutate(metric_label = if_else(metric == "r_squared", "Coefficient of Determination (R^2)", "Ratio of Coefficients (beta_1 / beta_2)"))

# Create the two-panel histogram plot
ggplot(plot_df, aes(x = value)) +
  geom_histogram(bins = 30, fill = "lightblue", color = "darkblue", alpha = 0.7) +
  facet_wrap(~ metric_label, scales = "free") +
  labs(
    title = "Bootstrap Distributions of R-squared and the Beta Ratio",
    x = "Estimated Value",
    y = "Count"
  ) +
  theme_minimal() +
  theme(plot.title.position = "plot")

ggsave("bootstrap_distributions.png", width = 10, height = 5)
```

### **Description of the Distributions**

  * **Coefficient of Determination ($\hat{r}^2$):** The distribution of $\hat{r}^2$ is **highly concentrated** and appears **skewed slightly to the left (negatively skewed)**. The bulk of the estimates falls in a narrow range around $0.935$ to $0.945$. This strong concentration indicates that the linear model consistently explains a very high proportion of the variance in $\text{tmax}$ across the different bootstrap samples.

  * **Ratio of Coefficients ($\hat{\beta}_1 / \hat{\beta}_2$):** The distribution of the ratio is **much wider** and appears roughly **symmetrical and bell-shaped**, suggesting a distribution close to normal. The estimates are centered around a value close to $-150$. This wider spread, compared to $\hat{r}^2$, reflects greater sampling variability for the ratio of two estimated regression parameters.

-----

## Problem 2 Part 4:  **$95\%$ Confidence Intervals**

The $95\%$ confidence intervals are obtained by finding the $2.5\%$ and $97.5\%$ quantiles of the 5000 bootstrap estimates.

```{r}
# Calculate the 2.5% and 97.5% quantiles for both metrics
confidence_intervals = boot_results |>
  summarise(
    r2_ci_low = quantile(r_squared, 0.025),
    r2_ci_high = quantile(r_squared, 0.975),
    ratio_ci_low = quantile(beta_ratio, 0.025),
    ratio_ci_high = quantile(beta_ratio, 0.975)
  )

confidence_intervals
```

| Metric | $2.5\%$ Quantile (Lower Bound) | $97.5\%$ Quantile (Upper Bound) |
| :--- | :--- | :--- |
| $\hat{r}^2$ | $\mathbf{0.935}$ | $\mathbf{0.947}$ |
| $\hat{\beta}_1 / \hat{\beta}_2$ | $\mathbf{-280.8434}$ | $\mathbf{-125.0654}$ |

**Conclusion:**

  * The **$95\%$ confidence interval for $\hat{r}^2$** is $(\mathbf{0.935}, \mathbf{0.947})$.
  * The **$95\%$ confidence interval for $\hat{\beta}_1 / \hat{\beta}_2$** is $(\mathbf{-280.8434}, \mathbf{-125.0654})$.
  

# Problem 3




